---
title: "HW6 Telemarketing"
author: "Group 7 Nihal Kurki, Nelvin Vincent, Wendy Liu & Mack Khoo"
date: "3/22/2020"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
#summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
# 0 = never called, 1 = called before
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

#str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
#str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```

## KMeans Clustering
```{r}
set.seed(12345)

tele_predictors <- tele_norm[, -match("yyes", names(tele_norm))]
#interests_z <- as.data.frame(lapply(interests, scale))
tele_labels <- tele_norm[, match("yyes", names(tele_norm))]

tele_clusters <- kmeans(tele_predictors, 5)

# look at the size of the clusters
tele_clusters$size

# look at the cluster centers
tele_clusters$centers

# tele_predictors
tele_predictors$cluster <- tele_clusters$cluster
tele_predictors$yyes <- tele_labels

clustertable <- aggregate(data = tele_predictors, yyes ~ cluster, mean)
clustertable

tele_cluster1 <- tele_predictors[tele_predictors$cluster == 1, ]
tele_cluster1$cluster <- NULL
tele_cluster2 <- tele_predictors[tele_predictors$cluster == 2, ]
tele_cluster2$cluster <- NULL
tele_cluster3 <- tele_predictors[tele_predictors$cluster == 3, ]
tele_cluster3$cluster <- NULL
tele_cluster5 <- tele_predictors[tele_predictors$cluster == 5, ]
tele_cluster5$cluster <- NULL

#tele_predictors$cluster4 <- (tele_predictors$cluster == 4)
#tele_predictors$cluster4 <- ifelse(tele_predictors$cluster4 == "TRUE", 1,0)

# calculate how many individuals belong to cluster 4
sum(tele_predictors$cluster == 4)

# calculate how many percent of individuals in cluster actually purchase
(sum(tele_predictors$cluster == 4))*(0.23184323)
```

## Getting Train and Test Samples (KNN) Cluster 1

```{r}
# Selects 10000 random rows for test data
test_set1 <- sample(1:nrow(tele_cluster1), 2500)
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_cluster1[-test_set1, -match("yyes",names(tele_cluster1))]
tele_test <- tele_cluster1[test_set1, -match("yyes",names(tele_cluster1))]

# tele_train$yyes <- NULL
# tele_test$yyes <- NULL

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_cluster1[-test_set1, "yyes"]
tele_test_labels <- tele_cluster1[test_set1, "yyes"]

#Lets run the KNN command
library(class)
library(caret)

#Run KNN on train data, create predictions for test data
#Starting K value close to sqrt(nrow(alcohol2_train))
sqrt(nrow(tele_train))

KNN_tele_model <- knn(train = tele_train, test = tele_test,
                      cl = tele_train_labels, k=3)

KNN_prediction <- KNN_tele_model
KNN_yyes_pred_1 <- ifelse(KNN_tele_model == "0", 0, 1)

#Evaluate model results
library(gmodels)
CrossTable(x = tele_test_labels, y = KNN_tele_model, prop.chisq=FALSE)

confusionMatrix(as.factor(KNN_tele_model), as.factor(tele_test_labels), positive = "1")

```
> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## Build an ANN Model Cluster 1
```{r, cache = TRUE} 
test_set1 <- sample(1:nrow(tele_cluster1), 2500) 

tele_norm_train <- tele_cluster1[-test_set1, ]
tele_norm_test <- tele_cluster1[test_set1, ]

library(neuralnet)

# simple ANN with only a single hidden neuron
ANN_tele_model <- neuralnet(formula = yyes ~. ,
                              data = tele_norm_train)

model_results <- compute(ANN_tele_model, tele_norm_test[1:53])

predicted_y <- model_results$net.result

prediction <-predict(ANN_tele_model, tele_norm_test)

ANN_prediction <- prediction
ANN_yyes_pred_1 <- ifelse(prediction < 0.2, 0, 1)

library(gmodels)
library(caret)

CrossTable(x = tele_norm_test$yyes, y = ANN_yyes_pred_1, prop.chisq=FALSE)

confusionMatrix(as.factor(ANN_yyes_pred_1), as.factor(tele_norm_test$yyes), positive = "1")


```

## Logistic Regression Model Cluster 1

```{r}
#Set a seed for random number generator for consistent output
test_set <- sample(1:nrow(tele_cluster1), 2500) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 
# Create a train set and test set
#First the predictors
tele_train_log <- tele_cluster1[-test_set, ]
tele_test_log <- tele_cluster1[test_set, ]
library(gmodels)
library(caret)
firstmodel <- glm(yyes ~ age + jobblue.collar + jobservices + maritalsingle + defaultunknown +
                    contacttelephone + monthaug + 
                    monthdec + monthjun + monthmar + monthmay + monthnov + monthdec +
                    day_of_weekmon + day_of_weekwed + campaign + 
                    poutcomenonexistent + poutcomesuccess + emp.var.rate + 
                    cons.price.idx + cons.conf.idx + nr.employed + pdaysdummy + maritalsingle*age +
                    jobblue.collar*emp.var.rate + campaign*poutcomesuccess + campaign*nr.employed, 
                    data = tele_norm, family = "binomial")
#summary(firstmodel)

prediction <- predict(firstmodel, tele_test_log)
LOG_yyes_pred_1 <- ifelse(prediction < 0.1, 0, 1)
CrossTable(x = tele_test_log$yyes, y = LOG_yyes_pred_1, prop.chisq=FALSE)
confusionMatrix(as.factor(LOG_yyes_pred_1), as.factor(tele_test_log$yyes), positive = "1")


```
## Majority Vote Classification Cluster 1
```{r}
KNN <- KNN_yyes_pred_1 
ANN <- ANN_yyes_pred_1
LOG <- LOG_yyes_pred_1
all1 <- data.frame(KNN, ANN, LOG)
all1$all_prediction <- KNN_yyes_pred_1 + ANN_yyes_pred_1 + LOG_yyes_pred_1
all1$all_prediction <- ifelse(all1$all_prediction >= 1, 1, 0)

CrossTable(x = tele_test_log$yyes, y = all1$all_prediction, prop.chisq=FALSE)
confusionMatrix(as.factor(all1$all_prediction), as.factor(tele_test_log$yyes), positive = "1")
```

## Getting Train and Test Samples (KNN) Cluster 2

```{r}
# Selects 10000 random rows for test data
test_set2 <- sample(1:nrow(tele_cluster2), 2500)
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_cluster2[-test_set2, -match("yyes",names(tele_cluster2))]
tele_test <- tele_cluster2[test_set2, -match("yyes",names(tele_cluster2))]

# tele_train$yyes <- NULL
# tele_test$yyes <- NULL

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_cluster2[-test_set2, "yyes"]
tele_test_labels <- tele_cluster2[test_set2, "yyes"]

#Lets run the KNN command
library(class)
library(caret)

#Run KNN on train data, create predictions for test data
#Starting K value close to sqrt(nrow(alcohol2_train))
sqrt(nrow(tele_train))

KNN_tele_model <- knn(train = tele_train, test = tele_test,
                      cl = tele_train_labels, k=3)

KNN_prediction <- KNN_tele_model
KNN_yyes_pred_2 <- ifelse(KNN_tele_model == "0", 0, 1)

#Evaluate model results
library(gmodels)
CrossTable(x = tele_test_labels, y = KNN_tele_model, prop.chisq=FALSE)

confusionMatrix(as.factor(KNN_tele_model), as.factor(tele_test_labels), positive = "1")

```
> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## Build an ANN Model Cluster 2
```{r, cache = TRUE} 
test_set2 <- sample(1:nrow(tele_cluster2), 2500) 

tele_norm_train <- tele_cluster2[-test_set2, ]
tele_norm_test <- tele_cluster2[test_set2, ]

library(neuralnet)

# simple ANN with only a single hidden neuron
ANN_tele_model <- neuralnet(formula = yyes ~. ,
                              data = tele_norm_train)

model_results <- compute(ANN_tele_model, tele_norm_test[1:53])

predicted_y <- model_results$net.result

prediction <-predict(ANN_tele_model, tele_norm_test)

ANN_prediction <- prediction
ANN_yyes_pred_2 <- ifelse(prediction < 0.5, 0, 1)

library(gmodels)
library(caret)

CrossTable(x = tele_norm_test$yyes, y = ANN_yyes_pred_2, prop.chisq=FALSE)

confusionMatrix(as.factor(ANN_yyes_pred_2), as.factor(tele_norm_test$yyes), positive = "1")


```

## Logistic Regression Model Cluster 2

```{r}
#Set a seed for random number generator for consistent output
test_set <- sample(1:nrow(tele_cluster2), 2500) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 
# Create a train set and test set
#First the predictors
tele_train_log <- tele_cluster2[-test_set, ]
tele_test_log <- tele_cluster2[test_set, ]
library(gmodels)
library(caret)
firstmodel <- glm(yyes ~ age + jobblue.collar + jobservices + maritalsingle + defaultunknown +
                    contacttelephone + monthaug + 
                    monthdec + monthjun + monthmar + monthmay + monthnov + monthdec +
                    day_of_weekmon + day_of_weekwed + campaign + 
                    poutcomenonexistent + poutcomesuccess + emp.var.rate + 
                    cons.price.idx + cons.conf.idx + nr.employed + pdaysdummy + maritalsingle*age +
                    jobblue.collar*emp.var.rate + campaign*poutcomesuccess + campaign*nr.employed, 
                    data = tele_norm, family = "binomial")
#summary(firstmodel)

prediction <- predict(firstmodel, tele_test_log)
LOG_prediction <- prediction
LOG_yyes_pred_2 <- ifelse(prediction < 0.1, 0, 1)
CrossTable(x = tele_test_log$yyes, y = LOG_yyes_pred_2, prop.chisq=FALSE)
confusionMatrix(as.factor(LOG_yyes_pred_2), as.factor(tele_test_log$yyes), positive = "1")


```
## Majority Vote Classification Cluster 2
```{r}
KNN <- KNN_yyes_pred_2 
ANN <- ANN_yyes_pred_2
LOG <- LOG_yyes_pred_2
all2 <- data.frame(KNN, ANN, LOG)
all2$all_prediction <- KNN_yyes_pred_2 + ANN_yyes_pred_2 + LOG_yyes_pred_2
all2$all_prediction <- ifelse(all2$all_prediction >= 1, 1, 0)

CrossTable(x = tele_test_log$yyes, y = all2$all_prediction, prop.chisq=FALSE)
confusionMatrix(as.factor(all2$all_prediction), as.factor(tele_test_log$yyes), positive = "1")

```

## Getting Train and Test Samples (KNN) Cluster 3

```{r}
# Selects 10000 random rows for test data
test_set3 <- sample(1:nrow(tele_cluster3), 2500)
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_cluster3[-test_set3, -match("yyes",names(tele_cluster3))]
tele_test <- tele_cluster3[test_set3, -match("yyes",names(tele_cluster3))]

# tele_train$yyes <- NULL
# tele_test$yyes <- NULL

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_cluster3[-test_set3, "yyes"]
tele_test_labels <- tele_cluster3[test_set3, "yyes"]

#Lets run the KNN command
library(class)
library(caret)

#Run KNN on train data, create predictions for test data
#Starting K value close to sqrt(nrow(alcohol2_train))
sqrt(nrow(tele_train))

KNN_tele_model <- knn(train = tele_train, test = tele_test,
                      cl = tele_train_labels, k=7)

KNN_prediction <- KNN_tele_model
KNN_yyes_pred_3 <- ifelse(KNN_tele_model == "0", 0, 1)

#Evaluate model results
library(gmodels)
CrossTable(x = tele_test_labels, y = KNN_tele_model, prop.chisq=FALSE)

confusionMatrix(as.factor(KNN_tele_model), as.factor(tele_test_labels), positive = "1")

```
> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## Build an ANN Model Cluster 3
```{r, cache = TRUE} 
test_set3 <- sample(1:nrow(tele_cluster3), 2500) 

tele_norm_train <- tele_cluster3[-test_set3, ]
tele_norm_test <- tele_cluster3[test_set3, ]

library(neuralnet)

# simple ANN with only a single hidden neuron
ANN_tele_model <- neuralnet(formula = yyes ~. ,
                              data = tele_norm_train)

model_results <- compute(ANN_tele_model, tele_norm_test[1:53])

predicted_y <- model_results$net.result

prediction <-predict(ANN_tele_model, tele_norm_test)

ANN_prediction <- prediction
ANN_yyes_pred_3 <- ifelse(prediction < 0.3, 0, 1)

library(gmodels)
library(caret)

CrossTable(x = tele_norm_test$yyes, y = ANN_yyes_pred_3, prop.chisq=FALSE)

confusionMatrix(as.factor(ANN_yyes_pred_3), as.factor(tele_norm_test$yyes), positive = "1")


```

## Logistic Regression Model Cluster 3

```{r}
#Set a seed for random number generator for consistent output
test_set <- sample(1:nrow(tele_cluster2), 2500) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 
# Create a train set and test set
#First the predictors
tele_train_log <- tele_cluster3[-test_set, ]
tele_test_log <- tele_cluster3[test_set, ]
library(gmodels)
library(caret)
firstmodel <- glm(yyes ~ age + jobblue.collar + jobservices + maritalsingle + defaultunknown +
                    contacttelephone + monthaug + 
                    monthdec + monthjun + monthmar + monthmay + monthnov + monthdec +
                    day_of_weekmon + day_of_weekwed + campaign + 
                    poutcomenonexistent + poutcomesuccess + emp.var.rate + 
                    cons.price.idx + cons.conf.idx + nr.employed + pdaysdummy + maritalsingle*age +
                    jobblue.collar*emp.var.rate + campaign*poutcomesuccess + campaign*nr.employed, 
                    data = tele_norm, family = "binomial")
#summary(firstmodel)

prediction <- predict(firstmodel, tele_test_log)
LOG_prediction <- prediction
LOG_yyes_pred_3 <- ifelse(prediction < 0.01, 0, 1)
CrossTable(x = tele_test_log$yyes, y = LOG_yyes_pred_3, prop.chisq=FALSE)
confusionMatrix(as.factor(LOG_yyes_pred_3), as.factor(tele_test_log$yyes), positive = "1")


```
## Majority Vote Classification Cluster 3
```{r}
KNN <- KNN_yyes_pred_3 
ANN <- ANN_yyes_pred_3
LOG <- LOG_yyes_pred_3
all3 <- data.frame(KNN, ANN, LOG)
all3$all_prediction <- KNN_yyes_pred_3 + ANN_yyes_pred_3 + LOG_yyes_pred_3
all3$all_prediction <- ifelse(all3$all_prediction >= 1, 1, 0)

CrossTable(x = tele_test_log$yyes, y = all3$all_prediction, prop.chisq=FALSE)
confusionMatrix(as.factor(all3$all_prediction), as.factor(tele_test_log$yyes), positive = "1")

```

## Getting Train and Test Samples (KNN) Cluster 5

```{r}
# Selects 10000 random rows for test data
test_set5 <- sample(1:nrow(tele_cluster5), 1000)
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_cluster5[-test_set5, -match("yyes",names(tele_cluster5))]
tele_test <- tele_cluster5[test_set5, -match("yyes",names(tele_cluster5))]

# tele_train$yyes <- NULL
# tele_test$yyes <- NULL

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_cluster5[-test_set5, "yyes"]
tele_test_labels <- tele_cluster5[test_set5, "yyes"]

#Lets run the KNN command
library(class)
library(caret)

#Run KNN on train data, create predictions for test data
#Starting K value close to sqrt(nrow(alcohol2_train))
sqrt(nrow(tele_train))

KNN_tele_model <- knn(train = tele_train, test = tele_test,
                      cl = tele_train_labels, k=3)

KNN_prediction <- KNN_tele_model
KNN_yyes_pred_5 <- ifelse(KNN_tele_model == "0", 0, 1)

#Evaluate model results
library(gmodels)
CrossTable(x = tele_test_labels, y = KNN_tele_model, prop.chisq=FALSE)

confusionMatrix(as.factor(KNN_tele_model), as.factor(tele_test_labels), positive = "1")

```
> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## Build an ANN Model Cluster 5
```{r, cache = TRUE} 
test_set5 <- sample(1:nrow(tele_cluster5), 1000) 

tele_norm_train <- tele_cluster5[-test_set5, ]
tele_norm_test <- tele_cluster5[test_set5, ]

library(neuralnet)

# simple ANN with only a single hidden neuron
ANN_tele_model <- neuralnet(formula = yyes ~. ,
                              data = tele_norm_train)

model_results <- compute(ANN_tele_model, tele_norm_test[1:53])

predicted_y <- model_results$net.result

prediction <-predict(ANN_tele_model, tele_norm_test)

ANN_prediction <- prediction
ANN_yyes_pred_5 <- ifelse(prediction < 0.15, 0, 1)

library(gmodels)
library(caret)

CrossTable(x = tele_norm_test$yyes, y = ANN_yyes_pred_5, prop.chisq=FALSE)

confusionMatrix(as.factor(ANN_yyes_pred_5), as.factor(tele_norm_test$yyes), positive = "1")


```

## Logistic Regression Model Cluster 5

```{r}
#Set a seed for random number generator for consistent output
test_set <- sample(1:nrow(tele_cluster5), 1000) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 
# Create a train set and test set
#First the predictors
tele_train_log <- tele_cluster5[-test_set, ]
tele_test_log <- tele_cluster5[test_set, ]
library(gmodels)
library(caret)
firstmodel <- glm(yyes ~ age + jobblue.collar + jobservices + maritalsingle + defaultunknown +
                    contacttelephone + monthaug + 
                    monthdec + monthjun + monthmar + monthmay + monthnov + monthdec +
                    day_of_weekmon + day_of_weekwed + campaign + 
                    poutcomenonexistent + poutcomesuccess + emp.var.rate + 
                    cons.price.idx + cons.conf.idx + nr.employed + pdaysdummy + maritalsingle*age +
                    jobblue.collar*emp.var.rate + campaign*poutcomesuccess + campaign*nr.employed, 
                    data = tele_norm, family = "binomial")
#summary(firstmodel)

prediction <- predict(firstmodel, tele_test_log)
LOG_prediction <- prediction
LOG_yyes_pred_5 <- ifelse(prediction < 0.01, 0, 1)
CrossTable(x = tele_test_log$yyes, y = LOG_yyes_pred_5, prop.chisq=FALSE)
confusionMatrix(as.factor(LOG_yyes_pred_5), as.factor(tele_test_log$yyes), positive = "1")


```
## Majority Vote Classification Cluster 5
```{r}
KNN <- KNN_yyes_pred_5 
ANN <- ANN_yyes_pred_5
LOG <- LOG_yyes_pred_5
all5 <- data.frame(KNN, ANN, LOG)
all5$all_prediction <- KNN_yyes_pred_5 + ANN_yyes_pred_5 + LOG_yyes_pred_5
all5$all_prediction <- ifelse(all5$all_prediction >= 1, 1, 0)

CrossTable(x = tele_test_log$yyes, y = all5$all_prediction, prop.chisq=FALSE)
confusionMatrix(as.factor(all5$all_prediction), as.factor(tele_test_log$yyes), positive = "1")

```


## Conclusion

- ANN models are better due to success % but we wanted to use combined to get the most calls
- Also wanted to lower false negative rate because that is revenue lost
- We can see with all the models, clusters 1 and 2 are very poor, so consider not calling them
- Call everyone in cluster 4
- In clusters 3 and 5 use the predictive models created to call with success rate of > 20% which is profitable

Current State:
41k
5k success
-> 41000*(-1) + 5000(6) = -11000

To Be State:
Per 100 customers in group 4
8165/21274 (38.4) is the percentage of people we can expect in cluster 4
0.23184323 is the buy rate
8.9 buy


Per 100 customers in group 3
9653/21274 (45.4) is the percentage of people we can expect in cluster 3
.4787 is the buy rate
21.7 buy

Per 100 customers in group 5
3456/21274 (16.2) is the percentage of people we can expect in cluster 5
.47 is the buy rate
7.6 buy

total out of 100 -> 38.2 buy

for 41k calls in the to be state:
-> 41000(-1) + 15662(6) = 52972 profit using the ANN model on clusters 3 and 5 and calling all in 4
                          13366 profit using voting scheme on clusters 3 and 5 and calling all in 4